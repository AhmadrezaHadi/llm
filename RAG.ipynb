{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pnaghs1\\AppData\\Local\\anaconda\\envs\\LLM\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Failed to patch SmolVLMForConditionalGeneration forward function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.7.0+cu126 with CUDA 1206 (you have 2.7.0+cu118)\n",
      "    Python  3.10.11 (you have 3.10.17)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pnaghs1\\AppData\\Local\\anaconda\\envs\\LLM\\lib\\site-packages\\unsloth_zoo\\gradient_checkpointing.py:330: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  GPU_BUFFERS = tuple([torch.empty(2*256*2048, dtype = dtype, device = f\"cuda:{i}\") for i in range(n_gpus)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.1: Fast Llama patching. Transformers: 4.51.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3050 OEM. Num GPUs = 1. Max memory: 8.0 GB. Platform: Windows.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu118. CUDA: 8.6. CUDA Toolkit: 11.8. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot set gray non-stroke color because /'p6' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'p7' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'p8' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'p9' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'p10' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'p11' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'p12' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'p13' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'p14' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'p15' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'p17' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'p6' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'p7' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'p8' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'p9' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'p10' is an invalid float value\n",
      "Cannot set gray non-stroke color because /'p12' is an invalid float value\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 2. Load a PDF from local or web\n",
    "from pdfminer.high_level import extract_text\n",
    "pdf_path = \"d2l-en.pdf\"  # change as needed\n",
    "text = extract_text(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Split text into chunks\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = text_splitter.create_documents([text])\n",
    "\n",
    "# 4. Embed chunks into a Vectorstore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "\n",
    "embedding = GPT4AllEmbeddings()\n",
    "vectordb = Chroma.from_documents(documents=chunks, embedding=embedding, persist_directory=\"rag_store\")\n",
    "\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pnaghs1\\AppData\\Local\\Temp\\ipykernel_50556\\1729965260.py:3: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Ask a question and retrieve context\n",
    "query = \"What is the code for AlexNet controls the model complexity ?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "# 6. Build final prompt\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "final_prompt = f\"\"\"Answer the following question using the context below:\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Answer the following question using the context below:\n",
      "Context:\n",
      "Although it seems that there are only a few more lines in AlexNetâ€™s implementation than\n",
      "in LeNetâ€™s, it took the academic community many years to embrace this conceptual change\n",
      "and take advantage of its excellent experimental results. This was also due to the lack of\n",
      "efficient computational tools. At the time neither DistBelief (Dean et al., 2012) nor Caffe\n",
      "(Jia et al., 2014) existed, and Theano (Bergstra et al., 2010) still lacked many distinguishing\n",
      "\n",
      "Capacity Control and Preprocessing\n",
      "\n",
      "AlexNet controls the model complexity of the fully connected layer by dropout (Section\n",
      "5.6), while LeNet only uses weight decay. To augment the data even further, the training\n",
      "loop of AlexNet added a great deal of image augmentation, such as flipping, clipping, and\n",
      "color changes. This makes the model more robust and the larger sample size effectively\n",
      "reduces overfitting. See Buslaev et al. (2020) for an in-depth review of such preprocessing\n",
      "steps.\n",
      "\n",
      "Model Complexity\n",
      "\n",
      "Question:\n",
      "What is the code for AlexNet controls the model complexity?\n",
      "What is the code for AlexNet controls the model complexity?\n",
      "\n",
      "AlexNet controls the model complexity of the fully connected layer by dropout (Section 5.6). The code for AlexNet is:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "from tensorflow.keras import layers\n",
      "\n",
      "def create_AlexNet():\n",
      "    # Input layer\n",
      "    input_layer = layers.Input(shape=(28, 28, 1))\n",
      "\n",
      "    # Convolutional block 1\n",
      "    conv_block_1 = layers.Conv2D(32, kernel_size=3, padding='same')(input_layer)\n",
      "    conv_block_1 = layers.MaxPooling2D(pool_size=(2, 2))(conv_block_1)\n",
      "\n",
      "    # Convolutional block 2\n",
      "    conv_block_2 = layers.Conv2D(64, kernel_size=3, padding='same')(conv_block_1)\n",
      "    conv_block_2 = layers.MaxPooling2D(pool_size=(2, 2))(conv_block_2)\n",
      "\n",
      "    # Convolutional block 3\n",
      "    conv_block_3 = layers.Conv2D(128, kernel_size=3, padding='same')(conv_block_2)\n",
      "    conv_block_3 = layers.MaxPooling2D(pool_size=(2, 2))(conv_block_3)\n",
      "\n",
      "    # Flatten\n",
      "    flatten = layers.Flatten()(conv_block_3)\n",
      "\n",
      "    # Dense layer\n",
      "    dense_layer = layers.Dense(512, activation='relu')(flatten)\n",
      "\n",
      "    # Dropout\n",
      "    dropout = layers.Dropout(0.5)(dense_layer)\n",
      "\n",
      "    # Output layer\n",
      "    output_layer = layers.Dense(10, activation='softmax')(dropout)\n",
      "\n",
      "    return output_layer\n",
      "```\n",
      "\n",
      "This code defines a function `create_AlexNet()` that creates a neural network architecture similar to AlexNet. It includes multiple convolutional blocks followed by a flattening layer, a dense layer, a dropout layer, and finally an output layer with softmax activation. Note that this is a simplified version of AlexNet's architecture and does not include some features like batch normalization or pre-trained weights.\n",
      "\n",
      "The final answer is: There is no single numerical answer to this problem, as it involves creating a neural network architecture rather than solving a numerical problem. However, I can provide you with the code for creating AlexNet's architecture.\n",
      "\n",
      "If you'd like, I can also help you modify the code to suit your specific needs. Just let me know what modifications you need!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 7. Run the model\n",
    "inputs = tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=800,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "        temperature=0.7,\n",
    "        repetition_penalty=1.1,\n",
    "    )\n",
    "\n",
    "# 8. Decode answer\n",
    "answer = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
